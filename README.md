# bert-sentiment

The model gives an accuracy of 0.92-0.93 on IMDB 50K Reviews Dataset.

BERT (Bidirectional Encoder Representations from Transformers) leverages a transformer-based neural network to understand and generate human-like language. BERT employs an encoder-only architecture. In the original Transformer architecture, there are both encoder and decoder modules. The decision to use an encoder-only architecture in BERT suggests a primary emphasis on understanding input sequences rather than generating output sequences.
BERT is designed to generate a language model so, only the encoder mechanism is used. Sequence of tokens are fed to the Transformer encoder. These tokens are first embedded into vectors and then processed in the neural network. The output is a sequence of vectors, each corresponding to an input token, providing contextualized representations.
![Streamlit - Google Chrome 4_20_2025 2_18_11 PM](https://github.com/user-attachments/assets/dc080e15-ccfb-49ab-93a7-bbd987ba08ca)
